<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta charset="utf-8" />
    <link href="../styles/main.css" rel="stylesheet" />
    <title>Computer Architecture</title>
    <link rel="stylesheet" href="../highlighjs/styles/railscasts.css" />
    <script src="../highlighjs/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>


    <script src="../jQuery/jquery-3.3.1.min.js"></script>
    <script src="../js/main.js"></script>
</head>

<body>
        <a href="../index.html" ><img id="home" src="../images/Logo.jpg" alt=""></a>

    <nav>
        <ul>
            <li><a class="links" href="java.html">Java</a>
                <a class="overviews" href="JavaOverview.html">Overview</a>
            </li>
            <li><a class="links" href="compArchitecture.html">Architecture</a>
                <a class="overviews" href="CAoverview.html">Overview</a>
            </li>
            <li><a class="links" href="networking.html">Networking</a>

                <a class="overviews" href="NetExamPrep.html">Overview</a>
            </li>
            <li><a class="links" href="databases.html">Databases</a>
                <a class="overviews" href="DBOverview.html">Overview</a>
            </li>
            <li><a class="links" href="operatingSystems.html">OS</a>
                <a class="overviews" href="OSOverview.html">Overview</a>
            </li>
            <li><a class="links" href="usabilityEngineering.html">Usability</a>
                <a class="overviews" href="UsabEngOverview.html">Overview</a>
            </li>
            <li><a class="links" href="algorithms.html">Algorithms</a>
                <a class="overviews" href="Algoverview.html">Overview</a>
            </li>

        </ul>
    </nav>
    <img id="ham" src="../images/utlities/hamburger.jpg" alt="">
    <header>
        <img src="../images/architecture/architecture.jpg" title="Computer Motherboard"
            alt="background theme for Computer Architecture showing a Motherboard">
        <h1>CA Overview</h1>

    </header>
    <div style="text-align: center; padding:0.5em; font-size: 1.3em;">
        <a href="https://www.youtube.com/channel/UC4ls2cPrXHfEO_oTHZcCclA/playlists">CHANNEL THAT HAS THE WHOLE
            COURSE</a><br>
        <a href="https://www.youtube.com/watch?v=ftek1dFpxxU">Video explaining the sample question with percentages of
            ALU BEQ, LW and SW</a><br>
        <a href="../downloadPDFS/Review-1.pdf" download="Review1.pdf">Questions and answers - Review 1 - MIPS
            code</a><br>
        <a href="../downloadPDFS/Review-2.pdf" download="Review2.pdf">Questions and answers - Review 2 - Pipelined
            processor, Direct mapped cache</a>
    </div>
    <main class="overview">
        <h1>Introduction &#8691;</h1>
        <section>
            <img src="../images/architecture/designPrinciple.JPG" alt="describes MIPS design principle" width="500px">

            <h2>Computer abstraction</h2>
            <ol class="overviewLists">
                <li>Design for <b>Moore's law </b>- rapid change</li>
                <li>Use <b>abstraction</b> to simplify design</li>
                <li>Make <b>common case fast</b>, on simple cases</li>
                <li>performance via <b>pipelining</b>, sequential pattern of parallelism</li>
                <li>performance via <b>Parallelism</b></li>
                <li>arrange into a <b>Hierarchy of memory</b></li>
                <li>Performance via <b>prediction</b>, good guesses</li>
                <li>Dependability via <b>redundancy</b>, redundant components incase failure</li>
            </ol>

            <ul class="overviewLists">
                <li><b>Datapath</b> - performs operations on data</li>
                <li><b>Control</b> - sequences datapath, memory access</li>
                <li><b>Cache memory</b></li>
            </ul>
        </section>

        <h1>Computer performance &#8691;</h1>
        <section>
            <h1>Metrics</h1>
            <ul class="overviewLists">
                <li><b>Response time</b> - time taken for task to complete</li>
                <li><b>Throughput </b>- total work done per unit time</li>
                <li><b>Elapsed time </b>- total time to complete a task, including processing, I/O, OS overhead idle
                    time</li>
                <li><b>CPU time</b> - time processing a given job no - I/O,other job shares</li>
                <li><b>CPU clocking </b>- clock cycle, within you should execute 1 instruction</li>
                <li><b>Clock period:</b> duration of a clock cycle</li>
                <li><b>Clock frequency (rate)</b>: cycles per second</li>
            </ul>
            <h2>Key Metrics</h2>
            <p><b>Response time and Throughput</b></p>
            <h2>Relative performance = 1/Execution time</h2>
            <p>X is n time faster than Y --- Y/X = n</p>
            <p>ex. time A = 10s , time B = 15s, 15/10 = 1.5 times is A faster than B</p>
            <h2>CPU performance</h2>
            <p> CPU time = CPU clock cycles X clock cycle time = CPU clock cycles/ clock rate</p>
            <p>Improved performance by reducing clock cycles and increasing clock rate , clock rate is clock speed
                measured in GHz, 1 GHz is 1 x 10^9, clock cycles are calculated - time x GHx x 10^9 , eg. 10 x 2 x 10^9
            </p>
            <p>total Clock cycles is (how many clock cycles per instruction ) X ( execution time ) x ( GHz) </p>
            <h2>Instruction count and cycles per instruction</h2>
            <p><b>IC = Instruction count</b></p>
            <p><b>Clock cycle </b> = IC X cycles per instruction </p>
            <p>CPU time = IC X CPI X Clock Cycle Time = IC X CPI / Clock Rate</p>
            <h2>Performance</h2>
            <p>CPU time = ( IC / program ) X (Clock cycles / IC ) X (seconds / clock cycle) </p>
            <p>Performance depends on : <br><b>Algorithm, language, compiler ISA </b>-- ( all affect IC and CPI )</p>
            <h2>Power</h2>
            <p>Power wall - to avoid drastically increasing power demand for a system they lowered voltage to 1V, then
                they struggled to reduce it further.</p>
            <p>Power = Capacitive load X voltage^2 X frequency </p>

            <p> <a href="https://www.igi-global.com/dictionary/power-wall/38994">power wall</a> - Is used to describe
                the limitation of CPU clock rate, CPU design, and CPU performance improvements due to
                the thermal and electrical power constraints, i.e., technological inability to use higher clock rates,
                more transistor switching elements, to use higher volumes of electrical energy and inability to maintain
                overall thermal stability</p>
        </section>

        <h1>MIPS ISA &#8691;</h1>
        <section>
            <img src="../images/architecture/CAobjective.JPG"
                alt="identify, understand and determine objectives for MIPS ISA" width="350px">

            <h2>MIPS registers 32 registers 32Bits each</h2>
            <img src="../images/architecture/RegisterUsage.JPG" alt="">
            <ul class="overviewLists">
                <li>&#36;zero - constant 0</li>
                <li>&#36;v0 - &#36;v1 - Syscall codes go here - integer functions / expression evaluations</li>
                <li>&#36;a0 - &#36;a3 - Arguments, &#36;a0 hold values for printing, also used for <b>passing arguments
                        to subroutines</b></li>
                <li>&#36;t0 -&#36;t9 - temporary registers / general purpose</li>
                <li>&#36;s0-&#36;s7 - Saved </li>
                <li>&#36;gp ( global p.), &#36;sp(stack p. can be used for nesting subroutine calls), &#36;fp(frame p.),
                    &#36;ra(return address)</li>
            </ul>

            <h2>Study logical and conditional instructions - slt, ori, sll, srl....</h2>
            <img src="../images/architecture/MIPS.JPG" alt="table view of most of MIPS ISA commands " width="750px">

            <h2>Memory operands</h2>
            <p>MIPS is Big Endian ( most significant byte last ), words are aligned in memory ( multiples of 4 ),
                byte-addressed , register are faster to access than memory.</p>

            <h2>ISA DESIGN FEATURES</h2>
            <p> <b>Make common case fast</b> -addi ( immediate ) ( adding 1 ) for loops</p>
            <p><b>Smaller is faster</b> - fast clock cycle time, smaller instruction size, num of registers all aid
                speed</p>
            <p><b>Simplicity favours regularity </b>- implementation is simpler and it enables higher performance at
                lower cost <br><br>
                <b>regularity</b> of instructions allows<b> Simplicity</b> in datapath and controller <br>
                <b> datapath </b>- composes of ALUs registers, muxes, control, memories, PC, buses
            </p>

            <ol>
                <li>Constant Instruction size of 32Bits - allows for simpler decoding hardware</li>
                <li>Only 3 instruction formats - ^</li>
                <li>Theres no complex instruction - only simple ones executed in about 1 clock cycle</li>
                <li>Regular register set - all registers are 32 bits and thers 32 register</li>
                <li>Small number of instructions - simpler compilers</li>
            </ol>
            <h2>R-Format Instructions 32 Bits</h2>
            <table>
                <tr>
                    <th>op</th>
                    <th>rs</th>
                    <th>rt</th>
                    <th>rd</th>
                    <th>shamt</th>
                    <th>funct</th>
                </tr>
                <tr>
                    <td>6bit operation code</td>
                    <td>5bit first source register</td>
                    <td>5bit second source register</td>
                    <td>5bit destination register</td>
                    <td>5bit Shift amount</td>
                    <td>6bit function code - extends opcode</td>
                </tr>
            </table>

            <p>I format is for immediate arithmetics and load/store, j - format is for jump, so every other command that
                isn't i or j, will fall into R-Format</p>
            <h2>I-Format Instructions 32 Bits</h2>
            <table>
                <tr>
                    <th>op</th>
                    <th>rs</th>
                    <th>rt</th>
                    <th>Constant or address</th>
                </tr>
                <tr>
                    <td>6bit operation code</td>
                    <td>5bit first source register</td>
                    <td>5bit second source register</td>
                    <td>16bit constant/address</td>
                </tr>
            </table>

            <h2>J-Format Instructions 32 Bits</h2>
            <table>
                <tr>
                    <th>op</th>
                    <th>Address</th>
                </tr>
                <tr>
                    <td>6bit operation code</td>
                    <td>26bit address</td>
                </tr>
            </table>
            <h2>AND OR, NOT</h2>
            <p>And- useful to mask bits ( selecting some bits and setting rest to 0 ) <br>OR - sets bits to 1 and
                leaves
                other unchanged <br>NOT- inverts bits</p>

            <h2>Branching</h2>
            <p>Why not BLT - its too complicated and would require extra clock cycles per instruction, instead it
                uses
                two basic instructions to implement the logic , e.g blt is translated to slti and bne , bge - slt/i(
                depending if comparing with register or immediate) and beq</p>
            <p>The more complex branch statements are composed of bne, beq, slt, sltu, slti, sltiu, </p>
            <p> e.g beq with immediate value - beq &#36;t6, 1 , procedure -- add &#36;1 , &#36;0,1 and beq &#36;1,
                &#36;t6</p>
            <p><b>Signed vs Unsigned</b> (slt - set on less than ). slt/slti - signed, sltu/sltui unsigned. They
                yield
                different results as values differ whether they are interpreted as signed/unsigned</p>
            <h2>Procedure calling</h2>
            <p><b>jal - jump and link</b> address of this instruction is in &#36;ra so it can be used to return, jr
                &#36;ra
                returns to the place jal was called from</p>
            <p><b>Leaf procedure</b> - when no calls/links are made to the procedure <br> <b>Non-leaf procedure</b>
                -
                when its calling another procedure ( jal ) </p>
            <h2>Supporting large constants</h2>
            <p>up to 16 bit constants are supported ( make common case fast ), but sometimes 32 bits are needed.
                Combination of lui and ori commands are used. <br> lui - copies 16bits to the left of rt ( from R/I
                format) and clears the right 16 bits <br> ori- combines the 2 16bits</p>
            <h2>Key differences between RISC and CISC</h2>
            <ul class="overviewLists">
                <li>The way memory is addressed</li>
                <li>Branches are executed</li>
                <li>Exceptions are handled</li>
                <li>RISC use less power ( in embedded systems )</li>
            </ul>
        </section>

        <h1>Computer Arithmetics &#8691;</h1>
        <section>
            <a href="../downloadPDFS/Optimized Multiplier-Divider Examples.pdf"> Examples of optimized multiplier and
                optimized divider</a>
            <h2>Unsigned / Two's Complement</h2>
            <p>unsigned 0 - 2^32 -1 <br> signed - 2^31 to + 2^31<br> <b>bit 31 is the sign bit - 1 negative, 0
                    positive</b></p>
            <p><b>Non-negative numbers have the same unsigned
                    and 2s-complement representation</b></p>
            <p> <b>To convert to Twos compliment</b> invert each binary digit, and then add 1 to the total</p>
            <h2>Addition</h2>
            <p>0+0 = 0 , overflow 0 || 0+1 = 1 , overflow = 0 || 1+1 = 0 , overflow = 1 || 0 + 1 + 1 = 1 + 1 || 1+1+1 =
                1 , overflow = 1</p>
            <h2>Subtraction twos complement</h2>
            <p>Change the number to Twos Complement, then perform addition e.g 6 - 2 will become 6 + (-2) <br> the
                result, if beginning with 0 is a positive answer,<br> if 1 then its negative ( you would need to compute
                twos complement again to find the magnitude of that number) </p>
            <a href="https://www.youtube.com/watch?v=h_fY-zSiMtY">
                <h2> Normal binary Subtraction ( used for division ) | video</h2>
            </a>
            <p> <b> Borrow - </b> when 0-1 occurs, find next 1 in the top num, change it to a 0 ( every 0 we skipped get
                two 1s ) the operation we borrowed for now becomes (1 + 1) - 1 = 1 </p>
            <p>1 - 0 = 1 || 0 - 1 = ( borrow ) 1 || 1-1 = 0 || 0 - 0 = 0</p>
            <h2>Multiplication</h2>
            <a href="https://www.youtube.com/watch?v=IeRm--sokdg"> Implementing Multiplication video</a><br>
            <a href="https://www.youtube.com/watch?v=-QghzKWAZ6o"> Improving Multiplication hardware video</a> <br>

            <p>terminology - multiplicand ( top num ) x multiplier . <br> looking at the <b>multiplier from right to
                    left</b>.
                if it is a 1, <b>copy</b> the multiplicand down, <b>shift</b> 1 bit to left. <br> If it <b> is a 0</b>,
                just shift
                1 but to left</p>
            <p>Then add the results ( make sure to keep add them correctly keeping the shifted layout ) </p>

            <h2>Division</h2>
            <a href="https://www.youtube.com/watch?v=gNEm5QCe0eU">Implementing division video</a>
            <a href="https://www.youtube.com/watch?v=7m6I7_3XdZ8">Improving division hardware</a>
            <p>the basic idea - "keep shifting to the right untill you can subtract to get a positive number"</p>
            <p>If you can perform normal subtraction ( as above ) so result is positive <br> - the result ( with 1 bit
                carried down from dividend) becomes the new dividend,<br> repeat the process until you cannot subtract
                to get a positive number anymore, thats the remainder</p>

            <h2>Decimal to binary</h2>
            <p>divide the number by 2 and add 1 if necessary , the goal is to find a number that can be multiplied by 2
                and added with 1 or 0 to get the original number.<br> e.g 76 = 38 * 2 + 0 ... 38 = 19 *2 + 0 .... 19 = 9
                * 2 + 1 ...</p>
            <p> Read the result ( the added 0s and 1s ) from the bottom up, that forms the binary number of the decimal
            </p>

            <h2>Decimal floating point to binary</h2>
            <p>turn the number before the point as above</p>
            <p>multiply the decimal point by 2 ( e,g 0.45 * 2 = 0.9 ) take the result and multiply it by 2.
                (0.9 * 2 = 1.8). if the result is 1.x , ignore the 1 when taking the result to next line. ( 0.8 * 2 =
                1.6).
            </p>
            <p>read the result from top down. Image below shows the steps</p>
            <img src="../images/architecture/decimal_to_binary.png" alt="method to convert decimal to binary">

            <h2>Single precision IEEE 754 Conversion</h2>
            <a href="https://www.youtube.com/watch?v=tx-M_rqhuUA">Video on how to convert, shows the two methods
                above</a>
            <p><b>Sign</b> - first bit , 0 if positive 1 if negative<br><b>Mantissa</b>- everything after the decimal
                point<br><b>Exponent</b> - the power value ( 2^5 , exponent is 5 ), the exponent comes from moving the
                decimal point to the front, the exponent value is how many places it was moved.
                <br><b>bias</b> a value of 127 that you add to the exponent ( then turn into binary ) and it gives you
                the final 8bit exponent
                <p>Now add 127 plus the exponent and change it to binary <br> this is the exponent, copy the mantissa
                    and
                    add the sign bit at the front. Thats IEE 754</p>

                <table>
                    <tr>
                        <th>S</th>
                        <th>Exponent - 8bits</th>
                        <th>fraction/mantissa 23bits</th>
                    </tr>
                </table>

                <h2>Floating point addition</h2>
                <a href="https://www.youtube.com/watch?v=mKJiD2ZAlwM">Video on how to add fps</a>
                <p>Change it to scientific notation. Make the exponents equal. add them together. convert to binary
                    representation if necessary.</p>
                <p>example: 1.000 x 2^-1 + -1.110 x 2^-2 <br> shift exponents:1.000 x 2^-1 + -0.111 x 2^-1 <br>add
                    significands: 0.001 x 2^-1 <br> normalize: 1.000 x 2^-4 </p>

                <ol>
                    <span>for a binary number</span>
                    <li>change the  8 bit binry number to decimal</li>
                    <li>take away 127 from it and that is your exponent ( the 2 to the power of..)</li>
                    <li>now write  1. X , where x is what ever comes after the exponent bit and x 2^exponent</li>
                    <span>Now for adding in format ( 1.0101 x 2^4 ) </span>
                    <li>make sure both number have the same exponent. </li>
                    <li>If not move the smaller one up to the bigger one by shifting the point to the left</li>
                    <li>then simply add 1.0101 to  the other number</li>
                </ol>
                <h2>Floating point multiplication</h2>
                <ol>
                    <span>Multiplying 1.000 x 2^-2 X 1.110 x 2^-2</span>
                    <ol>
                        <li>ex. 1.000 x 1.110</li>
                        <li>multiply 1000 x 1110 as normal</li>
                        <li>When you get the answer the decimal point goes into : x places from right to left</li>
                        <li>Where x is (number of digits after the point in multiplicand ) + (number of digits after point in multiplier)</li>
                        <li>in this case x is 6 (3+3)</li>
                        <li>to normalise move the point to the front but reflect the change in exponent.</li>

                    </ol>
                    
                </ol>
                <img src="../images/architecture/fp_multiplication.JPG" />
                <img src="../images/architecture/fp_binary_mul.JPG" />
                <img src="../images/architecture/fp_hardware.JPG" />
                <h2>FP commands</h2>
                <p>These operation are made on &#36;f0...&#36;f1... registers <br> <b>Commands:</b> - add.s , sub.s,
                    mul.s,
                    div.s ..
                    add.d, sub.d, mul.d, div.d</p>

                <img src="../images/architecture/fp_discussion.JPG" />
        </section>
        <h1>The Processor &#8691;</h1>
        <section>
            <h2>CPU overview</h2>
            <p><b>Two steps for instruction execution:</b> - instruction fetch and execution<br>
                <b>Instruction fetch-</b> PC &harr; instruction memory, PC &larr; target address <br>
                <b>Execution - </b> Read/write to registers, and use ALU to calculate for arithmetics or branching</p>

            <img src="../images/architecture/processor_components.JPG"
                alt="diagram showing different components in the processor" />
            <p><b>Clocking methodology </b>- transforms data druing clock cycles, between clock edges, the longest delay
                determines clock period </p>

            <h2>Datapath - components that process data and addresses in the CPU</h2>
            <p>datapath composes of ALUs registers, muxes, control, memories, PC, buses</p>
            <p>Load/store ( i format) - read/write values from registers.</p>
            <p><b>Branch instructions</b> i format, needs to read from registers and compare, and calculate target
                address.</p>

            <img src="../images/architecture/diagram.JPG" alt="diagram of datapath control" />
            <p>Different parts are turned on/off depending on what instruction is used (format and what the instruction
                does)</p>
            <p><b>Jumps - </b> they use word addresses, update the PC, opcode contains extra control signal that is set
                when opcode is 2.</p>
            <p> The instruction that is taking the longest time to execute, load instruction - critical path. It
                dictates the clock cycle
                <br> To improve Performance use pipelining</p>
            <h2>Pipelining 5 stages</h2>
            <ul class="overviewLists">
                <li>IF - instruction fetch from memory</li>
                <li>ID - instruction decode and register read</li>
                <li>EX - execute operation/calculate address</li>
                <li>MEM - access memory operand [not always ]</li>
                <li>WB - write result back to register [not always ]</li>
            </ul>

            <p><b>Pipeline design - </b> 32bit instruction size, few and regular formats, simple addressing for
                load/store and alignment of memory operands.</p>
            <p><b>Critical stage</b> - longest time operation is considered the critical stage
            </p>
            <p> if all stages are balanced - speedup = (time between instructions)pipelined / (time between
                instructions) single-cycle = number of pipeline stages</p>
            <p>if the stages aren't balanced the speed up is less, speedup increases Throughput</p>
            <h2>Pipeline Hazards</h2>
            <p><b>Structure hazard - </b> when two instructions need to access the same resource at the same time <br>
                &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; <i>Solution</i> - separate hardware (memory) for
                fetching ( they cause stalls/bubbles) <br>
                <b>Data hazard - </b> Data needed for an instruction but the instruction is still being processed <br>
                &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; <i>Solution</i> - forwarding- getting the result to
                the next instruction as soon as its done ( by hardware) ,avoids waiting for it to be stored in a
                register <br>
                <b> Control hazard - </b>Changing of execution flow disrupts the pipeline sequence<br>
                &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;<i>Solution</i> - branch predictions </p>
            <p>Forwarding doesn't always work, e.g when the value isn't computed when we need, still causes
                stalls/bubbles. <br><b>Software solution -</b> reorder code where stalls might occur. try not to use the
                same values ones if you can place another instruction in-between to make sure the value is computed by
                the time its needed</p>
            <p><b>Forwarding hardware -</b> to forward you read ALU operands from source other than register file
                registers, Multiplexers are needed at the ALU I/P, and a forwarding unit determines the multiplexor
                control signal</p>
            <img src="../images/architecture/table.JPG" alt="table explaining control codes" />
            <h2>Branch predictions - Control hazard solution</h2>
            <p>Important as for longer pipelines, the stall penalty is very high.</p>
            <p>With predictions you only stall when the prediction is wrong.In MIPS predict branches not taken</p>
            <p><a href="https://www.youtube.com/watch?v=71wplOXpQko&list=PL1C2GgOjAF-IWC1AEXqWKFmAgZdQRJfZ6&index=2"><b>Static
                        branch prediction</b></a>- based on the branch itself, simplest is to never take the branch, or
                always assume its taken.<br>Other options are - always take backwards branches, and not take forward
                branches. </p>
            <p><a href="https://www.youtube.com/watch?v=T8_Jvt2T6d0&list=PL1C2GgOjAF-IWC1AEXqWKFmAgZdQRJfZ6&index=3"><b>Dynamic
                        branch prediction </b></a>- making the prediction based on what happened last time.(requires
                hardware) and hope that it will continue the trend. Dynamic prediction also switches between static and
                dynamic, depending on the data it gather or a specific scenario </p>
            <p>Dynamic predictors are very popular as they are very accurate, sometimes over 90% accuracy.</p>

            <h2>Pipelined datapath hardware</h2>
            <img src="../images/architecture/datapath_pipelie.JPG"
                alt="picture shows diagram of the datapath pipeline hardware" />
        </section>

        <h1>The Processor (2) &#8691;</h1>
        <section>
            <h2>Forwarding, when to forward?</h2>
            <p> ID/EX.registerRs/Rt - instruction decode register ( RS is first source register ) stored in ID/EX (1)
            </p>
            <p></p>
            <p><b>When to forward - </b> ID/EX.RegisterRs/Rt = EX/MEM.RegisterRd or MEM/WB.Register.Rd <br> </p>
            <img src="../images/architecture/forwarding.JPG" alt="diagram showing stages in the pipeline" />
            <p>You should only forward when the instruction will write to a register, and if RD for instruction is not
                &#36;zero</p>
            <p><b>Double data hazard</b>- e.g add x ,X, y .. add X,x,z .. add x,X,w (capital X is where the double
                hazard occurs). In that case we need to use the most recent MEM stage result. <br> Only forward if EX
                hazard condition isn't true</p>
            <img src="../images/architecture/equations.JPG" alt="conditions on when to branch" />

            <h2>Memory load data hazard - hazard detection unit</h2>
            <p>ALU operand register numbers are given by IF/ID.RegisterRs/Rt. Load hazard occurs when <br>
                ID/EX.MemRead and <br>
                ((ID/EX.RegisterRt = IF/ID.RegisterRs) or <br>
                (ID/EX.RegisterRt = IF/ID.RegisterRt)) <br>
                if its detected a bubble is inserted
            </p>
            <p><b>How to stall the pipeline </b> - EX, MEM and WB perform a nop (no operation), PC and IF/ID are
                prevented from updating</p>
            <h2>Branch hazards Implementation ( hardware optimization )</h2>
            <p>Additional hardware is added to make branching decision at the instruction decode (ID) stage. <br>

            </p>
            <img src="../images/architecture/hazard_detection.JPG"
                alt="hazard detection unit diagram inside datapath pipeline diagram" />
            <img src="../images/architecture/hazard_examples.JPG" alt="data hazard stall examples for branches" />

            <h2>2bit predictor</h2>
            <img src="../images/architecture/2bit_predictor.JPG" alt="state diagram showing 2 bit predictor flow" />
            <p>it only changes prediction after two successive miss-predictions </p>
            <p><b>Calculating target address</b> The target buffers are in the cache, indexed by PC when instruction is
                fetched ( if branch taken, target addressed can be fetched immediately )</p>

            <h2>Exceptions and Interrupts</h2>
            <p><b>Exceptions -</b> arise within the CPU ( divide by 0, overflow, syscall, bad opcode ) <br> <b>Interrupt
                    -</b> from an external I/O. <br> Handling exceptions reduces performance</p>
            <p><b>Arithmetic overflow -</b> resulting value too large for the format <br>
                <b>Arithmetic underflow -</b> resulting value too small for the format <br>
                <b>Illegal instruction</b> - happens when Pc or memory was corrupted <br>
                <b>Bus error</b>- inaccessible resources <br>
                <b>Segmentation fault-</b> accessing memory in illegal ways</p>
            <p>Interrupts have a hardware-based solution for handling IO, When a device requires service it raises a
                flag, added hardware checks the flag. After detection the software decides whether to actually perform
                IO transaction</p>
            <h3>Interrupt Service routine (ISR)</h3>
            <p>Special OS program that is executed in the event of exception or interrupt. It stops the running of a
                program with a fatal program ( and report the problem)<br> for recoverable it resumes the running after
                handling the exception</p>
            <h3>Handling Exceptions/Interrupts , In the Kernel OS</h3>
            <ul class="overviewLists">
                <li>Saves current PC in special register</li>
                <li>Jumps to exception handler</li>
                <li>Saves the processor state</li>
                <li>deals with the exception/interrupt</li>
                <li>Resumes the processor state</li>
            </ul>
            <h3>Special registers - Co-processor 0</h3>
            <p><b>VAddr-</b> $8, offending memory reference <br>
                <b>Status</b>- $12, what interrupts are enabled | has an exception bit, to prevent the handler being
                interrupted <br>
                <b>Cause-</b> $13, Exception type and pending interrupts <br>
                <img src="../images/architecture/exceptions.JPG" /> <br>
                <b>EPC</b> - $14, PC - where the exception occurred.</p>
            <p>Handler always at address 0x80000180 in kernel memory <br>
                only $k0 and $k1 can be used freely, dont use the stack ( can point to invalid memory ). <br>
                Control to the user program can be restored with <b>eret</b></p>

            <img src="../images/architecture/pipeline_exception.JPG" />
        </section>
        <h1>The memory &#8691;</h1>
        <section>
            <h2>terminology</h2>
            <p><b>Non-Volatile memory</b>- keeps data without electrical power</p>
            <p>The list below is order from fastest to slowest</p>
            <ul class="overviewLists">

                <li><b>SRAM -</b>[CPU Cache] Static ram, keeps data as long as its powered</li>
                <li><b>DRAM - </b>[Main Memory] Dynamic RAM, Requires periodic refreshes to keep data | DDR double data
                    rate</li>
                <li><b>Flash memory- </b>retains data without power[EEPROM], USB drives and such </li>
                <li><b>Magnetic disc-</b> longest access time, large data</li>
            </ul>
            <h2>Memory Hierarchy</h2>
            <p>AS data is accessed from the disk, where everything is stored, the most recently accessed items are
                firstly stored on DRAM main memory, and then on cache ( smallest memory) <br>
                <b>DISK &#8594; Main Memory &#8594; Cache</b></p>

            <h2>principle of Locality</h2>
            <p>Programs access a small proportion of their address at any time.<br>
                <b>Temporal Locality</b> recently accessed items are likely to be accessed again <br>
                <b>Spatial locality</b> data closely together is likely to be accessed next. e.g processing an array</p>

            <h3>Four fundamental design questions</h3>

            <ol class="overviewLists">
                <li>Where can a block be placed?</li>
                <li>How is a block found?</li>
                <li>What block is replaced on a miss?</li>
                <li>How are writes handled?</li>
            </ol>
            <h2>Direct mapped cache</h2>
            <p>Data location in the cache is determined by Memory address <br>
                tags - high-order bits of the memory address used as index for the cache data table. <br>
                <b>Valid bit </b> when a block has valid data this bit is 1, else its 0.</p>
            <p><b>Cache size</b>- the cache stores data + tag + valid bit for every block <br>
                cache size = number of block x (block size + tag size + valid field size ) <br>tag size = address size -
                index size which is (n+m) | in mips = 32 - (n+m+2)</p>
            <p>Reported cache size is the data part only, omitting the space required for address and flags</p>
            <img src="../images/architecture/cache_example.JPG" />

            <h2>Block sizes / Cache miss or Hit</h2>
            <p>Larger block size &#8594; less blocks overall ( cache is small) &#8594; also bigger penalty ( more data
                to move ) <br> so whats the best block size to reduce cache miss. <br></p>
            <p><b>Cache miss - </b>
                <ol>
                    <li>Stall the CPU , PC - 4</li>
                    <li>Fetch the block from lower level Hierarchy (updates cache the tag and valid bit is 1) </li>
                    <li>Resume (if instruction cache miss, restart instruction fetch , if data miss complete the data
                        access)</li>
                </ol> <br>
                <b>Cache write - </b> <br><br>cache hit <br> <b>write-Through</b>(update both cache and next level
                memory)<br> <b>write-back</b>(update
                slower memory level when block is replaced , most complicated to implement but fastest) <br> <br> cache
                miss <br> <b> Write-allocate</b> (load the
                block in the cache and update the cache) <br>
                <b>no write allocate </b>- update the written address using write-through without loading the block.
                <br> <br> <b>Speeding up -</b> to speed things up ( as writing to memory takes around 100 cycles) , a
                write buffer is used, which holds the data, the CPU continues and only stalls if the buffer is already
                full. <br>
                <b> Dirty bit ( modified bit )</b> - hardware solution to tell if a block has been modified or not, its
                set when the processor writes to that block. This bit signifies that it needs to be updated across the
                hierarchy. <b> Page replacement</b> is closely related to the dirty bit.
            </p>
            <h2>Measuring Cache performance </h2>
            <img src="../images/architecture/performanceCache.JPG">
            <img src="../images/architecture/exampleCachePerformance.JPG">
            <p>Ideally we like , Small hit time, low miss ratio, small miss penalty <br>
                Avarage Memory access time ( AMAT ) | AMAT = hit time + miss rate X miss penalty <br>
            </p>

            <p>CPU clock = 1ns, Hit time = 1 cycle <br> miss penalty = 20 cycles, cache miss rate = 5% <br>
                AMAT = 1 + 0.05 X 20 = 2ns </p>
            <p><b>Performance concerns -</b> CPU performance increase can cause miss penalty to be more significant,
                decreasing CPI could increase STALLS.</p>

            <h2>Improving cache performance</h2>
            <p><b>Fully associative cache -</b> allow a given block to go in any cache entry( reduces competition), it
                increases search time unfortunately. <br>
                <b>N-way set-associative cache</b>- sets of n entries, block number determines which set, searches all
                entries in a given set ( less expensive to search)</p>
            <a href="https://en.wikipedia.org/wiki/Cache_placement_policies">Wikipedia page explaining direct, n-way and
                fully associative cache policies</a>
            <p>Increasing associativity decreases miss rate with diminishing returns <br>
                <b> Page Replacement policies </b>- Random / Least recently used (hard beyond 4-way as.) in set
                associative. Theres no choice in direct mapped.</p>
            <p><b>Reducing miss penalty</b>- Multi-level cache ( many cache hardware with different speeds (faster
                closer to CPU) but all faster than main memory) <br>
                Focus of L2 cache is to reduce access to main memory, overall reduces penalty. <br>
                Miss rates depend on software/algorithm/program behaviours and memory access patterns.</p>
            <img src="../images/architecture/MultilevelCache_example.JPG">
        </section>
        <h1>Virtual Memory &#8691;</h1>
        <section>
            <p>Using the main memory as "cache" for secondary disk storage <br>
                This is managed by CPU hardware and OS <br>
                Each program gets a share of the main memory ( private virtual address space holding its frequently used
                code ) <br>
                CPU and OS translate virtual addresses to physical addresses</p>
            <h2>Address translation</h2>
            <p>Disk is very slow, if a <b>Page fault occurs</b>(Virtual memory pages are blocks ) then it takes millions
                of clock cycles to get the data. <br> Page fault have to minimized ( fully associative placement,
                replacement algorithms ran by OS)</p>
            <p><b>Page Tables</b>- array of page table entries , each program has its own table.<br> if a page is in
                memory it stores physical page number + its status bits, if not reference to location on disk is kept
            </p>
            <img src="../images/architecture/PageTranslation.JPG" />
            <p><b>Replacement and writes</b> - least recently used (checking reference bit ),<br> disk writes use
                write-back ( as write through is impractical) when replacing a page ( dirty bit is set when a page is
                written)</p>
            <p><b>Translation Look-aside buffer (TLB)</b>- cache of Page table entries within the CPU</p>
            <img src="../images/architecture/TLB.JPG" />
            <p><b>Page fault handler (OS) -</b> finds PTE, locates page on disk, replaces a page(LRU), read page into
                memory and update table, run the process again ( restart from the faulting instruction)</p>
            <img src="../images/architecture/TLB_discussion.JPG" />

        </section>
        <h1>Parallel processing &#8691;</h1>
        <section>
            <h2>Power wall</h2>
            <p>Power wall refers to the inability to improve the performance and maintain
                low power consumption due to leak current in CMOS <br><br>
                Performance is improved by increasing the processor throughput
                (parallel processors) rather than response time (speeding the clock)</p>
            <h2>Hardware and software Parallelism</h2>
            <p>Sequential/concurrent software can run on serial/parallel hardware, the challenge is to make effective
                use of parallel hardware<br>
                it is difficult to make parallel programs - hard to partition the work into parallel pieces, Scheduling
                fairly, synchronization, minimal overhead</p>
            <img src="../images/architecture/parallel_example.JPG" />

            <img src="../images/architecture/parallelism_table.JPG" /><br>

            <a href="https://en.wikipedia.org/wiki/Flynn%27s_taxonomy">Flynn's taxonomy - SISD, MISD, SIMD, MIMD</a>
            <br>
            <h2>Instruction-level Parallelism (ILP)</h2>
            <p>Execution of multiiple instruction in parallel is ILP <br>
                <br> <b>Multiple issue: </b> <br>
                To get this we replicate the pipeline stages - multiple pipelines <br>
                multiple instructions per clock cycle <br>
                CPI becomes less than 1, so we use Instruction per cycle (IPC)
            </p>
            <p><b>Static Multiple issue -</b> [compiler based] detects and avoids hazards, packages into "issue
                slots"<br>
                <b>Dynamic multiple issue</b>[hardware based] CPU examines the instruction stream and chooses which
                instructions to issue each cycle , compiler helps by reordering instructions, hazards resolved with
                advanced techniques at runtime</p>

            <a href="../downloadPDFS/Lecture 21 - Parallel processing 2.pdf" download="lecture21.pdf">Lecture 21-Vector
                processors, vector instructions, multithreading -> different approaches, GPU architecture/operation</a>

        </section>

    </main>

    <aside>
        <h1>Things to remember</h1>
        <ol class="overviewLists">
            <li>if an instruction isn't i or j , the catch all is R-format</li>
            <li>128 is 1000 0000 , which is 1 more than the 127 bias.</li>
            <li>Response time and Throughput are the key metrics</li>
            <li>regularity of instructions allows Simplicity in datapath and controller</li>
            <li>32bit registers , regular instructions, simple load/store addresses, alignment of memory operand -
                <b>how MIPS is designed for pipelining</b></li>
            <li>Double data hazard and hazard detection unit are a thing ( processor 2)</li>
            <li>
                A translation lookaside buffer (TLB) is a memory cache that is used to reduce the time taken to access a
                user memory location.[1][2] It is a part of the chip’s memory-management unit (MMU). The TLB stores the
                recent translations of virtual memory to physical memory and can be called an address-translation cache.
            </li>

            <li>Flynns taxonomy is the classification of computer architectures. SSID SIMD MISD MIMD</li>


            <li>How to support Near and far jumps?
                <ol>
                    <li>I format support 16 bit address range</li>
                    <li>J-format support 26 bit address range</li>
                    <li>For really far jumps use jr, and save up to 32 bit address in a register</li>
                </ol>
            </li>

            <li>Techniques that can speed the pipelined processor.

                <p>
                    How is MIPS designed for pipelining:
                    <br>
                    Having a 32 bit fixed size instructions, favours fetching and decoding. <br>
                    You can read and decode an instruction in fixed number of clock cycles. <br>

                    Simple load/store addressing <br>

                    Alignment of memory operands. <br>

                    Few and regular instruction formats <br>
                </p>
            </li>

            <li>The runtime of a program is equal to the number of instructions multiplied by the average time per
                instruction. </li>
            <li>Amdahl law - is a formula which gives the theoretical speedup in latency of the execution of a task
                at fixed workload that can be expected of a system whose resources are improved</li>
        </ol>

        <h2>Speeding up pipeline processor</h2>
        <p><b>2018 autumn Q3 c:</b>
            <br>
            <br>
            <i>
                Idenitfy two techniques that can speed the pipelined processor. Explain how do they improve the
                performance
                and what are their performance limitations.
            </i>
        </p>

        <p>
            <b>so what affects the speed of a pipeline:</b> <br>
            <ol>
                <li>longest time ( the <b>critical stage</b>)</li>
                <li>time <b>between</b> Instructions</li>
                <li style="text-decoration: line-through;"> cache performance : miss penalty/miss rate ( not really that
                    relevant in that chapter)</li>
            </ol>

            so how to reduce the time of executions of the different stages <br><br>
            Avoiding all the hazards can speed up everything ( as hazards introduce stalls ).

            <ol class="overviewLists">
                <span><b>You can speed up pipeline by avoiding: </b></span> <br><br>
                <li><b>Structure hazards </b>- avoid stalls where two instructions want to access the same resource at a
                    time.
                    <b> Its limitation is</b> that dedicated hardware (fetching) is expensive and maybe perhaps the fact
                    that
                    once you have all the hardware you can completely avoid these stalls</li>

                <li><b>Data hazards - </b>stalling an instruction in the pipeline as it waits for data. Avoid this by
                    forwarding. <b>The limitation is</b> that you can only forward when the data is ready, you still
                    have to
                    stall if data is in the ALU</li>
                <li><b>Control hazard -</b> avoid flushing the pipeline when a branch is taken/not taken. this causes
                    significant performance decrease and you have to restart the pipeline with new instructions.
                    Introduce the branch prediction hardware (dynamic).<b> Limitation:</b> its expensive and you cannot
                    predict
                    the future but they are like 90% effective </li>


            </ol>

            all of these methods above reduce the time between instructions by removing the bubbles/stalls. This
            increases the pipeline performance. <br><br>

            <i style="text-decoration: line-through;">
                A stupid simple answer would be to get faster hardware with lower time for each stage. and to minimize
                cache
                miss and penalty.
            </i>
        </p>

        <h2>How to improve parallel processing</h2>
        <p>
            Due to the power wall (electrical demand and heat generation and physical barriers) we cannot increase the
            speed in single processors much more. <br>
            Multi-core processors use the idea of parallel processing.

            <br><br>
            Since parallel processing relies on dividing a task and computing it on separate cores and combining the
            results. One way to increase the speed is to have tasks that are suited for parallel processing.

            <br><br>
            <i>
                Communication and synchronization between the different subtasks are typically some of the greatest
                obstacles to getting good parallel program performance.
                .</i>
        </p>

        <h2>Cache Block size</h2>
        <img src="../images/architecture/CachBlockSize1.JPG" alt="">
        <img src="../images/architecture/CachBlockSize2.JPG" alt="">
        <img src="../images/architecture/CachBlockSize3.JPG" alt="">
    </aside>
</body>

</html>